\section{Solving Method}

Now there exist equations for which $C$ and $M$ can be solved, (\ref{equ:C_time_discret}) and (\ref{equ:M_space_discret} - \ref{equ:M_space_discret_boundary}) respectivly.
Using $C^{k}$ and $M^{k}_{i,j}$ as approximations of the solutions for (\ref{equ:model_system}) will allow the system to be solved by computing $C^{k+1}$ and $M^{k+1}_{i,j}$.
However, there are complications with trying to get an explicit formula for $M^{k+1}_{i,j}$ from (\ref{equ:M_space_discret} - \ref{equ:M_space_discret_boundary}) because of the dependency in $D(M)$ and $F(C)$.
To remedy this, a fixed point iteration is introduced.
In a single time step, the solutions for $M$ and $C$ can be solved using the previous time step solution in the follow manner:
\begin{equation} \label{equ:M_fixed_point}
  \frac{M^{(p+1)}_{i,j} - M^{k}_{i,j}}{\Delta t} = 
    \frac{1}{\Delta x^2} \sum_{(s,r) \in \mathbb{A}}
    \left( \frac{D(M^{p}_{i+s,j+r}) + D(M^{p}_{i,j})}{2} \cdot
    ( M^{(p+1)}_{i+s, j+r} - M^{(p+1)}_{i,j}) \right) 
    + F(C^{(p)}_{i,j}) M^{(p+1)}_{i,j}
\end{equation}
\begin{equation} \label{equ:C_fixed_point}
  \frac{C^{(p+1)} - C^{k}}{\Delta t} = \frac{-1}{2} ( G(C^{(p+1)}) M^{(p+1)} + G(C^{k}) M^{k} )
\end{equation}
%!% does p need to be (p) when not in an equation?
where $p \in (0,1,\ldots,P)$.
It is important to show explicitly the the purpose of the fixed point iteration is to link two distinct times with $P$ solutions in between them, such that:
\begin{equation}
  \begin{aligned}
  M^{(p=0)} &= M^{k}, \quad M^{(p=P)} = M^{k+1}, \\
  C^{(p=0)} &= C^{k}, \quad C^{(p=P)} = C^{k+1}.
  \end{aligned}
\end{equation}

In this fixed point formate, given by (\ref{equ:M_fixed_point} - \ref{equ:C_fixed_point}), the equations can be rearrange and solved by conventional methods.

For (\ref{equ:M_fixed_point}), a linear system of equations can be created following \cite{saad2003iterativeMethod}.
For each grid point $(i,j)$ a linear system exists, defined as:

\begin{equation} \label{equ:M_linear_equation}
\begin{aligned}
  \frac{M^{k}_{i,j}}{\Delta t} &= 
  \sum_{(i,j) \in \mathbb{A}} \left( \frac{D(M^{p+1}_{i+s,j+r}) + D(M^{p+1}_{i,j})} 
    {2\Delta x^2} \cdot M^{p+1}_{i+s, j+r} \right) \\
  & +\left( \sum_{(i,j) \in \mathbb{A}} \left( \frac{ D(M^{p+1}_{i+s,j+r}) + D(M^{p+1}_{i,j})} 
    {2\Delta x^2} \right) - F(C^{p}_{i,j}) + \frac{0}{\Delta t} \right) M^{p+1}_{i,j}.
\end{aligned}
\end{equation} 

From (\ref{equ:M_linear_equation}), a five-diagonal matrix can be created defined as,
%!% Some time find a better way to represent this matrix. Try copying Saad maybe?
\begin{equation} \label{equ:M_matrix_form}
  A = 
    \left( 
      \begin{array}{c c c c c c c c c c}
        a_{i,j} & a_{i+1,j} &  & a_{i,j+1} &   \\
        a_{i-1,j} & \ddots & \ddots &   &  \ddots &   \\
        & \ddots & \ddots & \ddots & & \ddots & \\
        a_{i,j-1} &  & a_{i-1,j} & a_{i,j} & a_{i+1,j} &   &  a_{i,j+1} &   \\
        & \ddots & & \ddots & \ddots & \ddots & & \ddots\\
        & & a_{i,j-1} &  & a_{i-1,j} & a_{i,j} & a_{i+1,j} &  & a_{i,j+1} \\
        & & & \ddots & & \ddots & \ddots & \ddots & \\
        & & & & \ddots & & \ddots & \ddots & a_{i+1,j} \\
        & & & & & a_{i,j-1} & & a_{i-1,j} & a_{i,j}
      \end{array}
    \right)
\end{equation}
where each $a_{i,j}$ is the coefficient based on (\ref{equ:M_linear_equation}). 

Solving (\ref{equ:M_matrix_form}) can be done by use of the Conjugate Gradient method provided that certain conditions are satisfied.

\begin{prop}
  The matrix $A$, defined in (\ref{equ:M_matrix_form}) is positive definite and symmetric when $\frac{1}{ F(C^{(p)}_{i,j}) } < \Delta t$.
\end{prop}
\begin{proof}
  Matrix $A$ is positive definite if all the eigenvalues are positive. 
  Using the Circle theorem described by \cite{gerschgorin1931uber_die_abgrenzung}, the eigenvalues can be shown to be positive if, independently on all rows, the sum of the off-diagonals values is less then the diagonal value.
  This can be verified. From (\ref{equ:M_linear_equation}) it can be said that,
  \begin{equation}
    \sum_{(i,j) \in \mathbb{A}} \left( \frac{D( M^{p}_{i+\frac{s}{2}, j+\frac{r}{2}} )}{\Delta x^2} \right)
     < \left( \sum_{(i,j) \in \mathbb{A}} \left( \frac{D( M^{p}_{i+\frac{s}{2}, j+\frac{r}{2}} )}{\Delta x^2} \right) 
    - F(C^{p}_{i,j}) + \frac{1}{\Delta t} \right). 
  \end{equation}
  This simplifies to,
  \begin{equation}
    F(C^{p}_{i,j}) < \frac{1}{\Delta t}
  \end{equation}

  The symmetry of $A$ can be trivially shown if one considers the formation of the diagonals.
  On a single row, each element corresponds to the adjacent grid points of grid $i,j$.
  As the grid ordering counts along, the elements that are equidistance from the diagonal are actually reference to the same grid point. 
  Therefore we have symmetry. 
\end{proof} 

It is important to remark that even though there is a condition for which matrix $A$ is positive definite and symmetric, it realistically will never occur.
The condition, $\frac{1}{F(C)} < \Delta t$, relates the growth of the biomass to the size of timestep selected.
Specifically, if a large enough time step is choosen, then $A$ is not guarenteed to converge.
When this occurs, it means that a time step, larger then the characteristic growth rate of the biomass, has been incorrectly choosen.
This means that the there would be no relavent results since all the growth, and subsequentally reactions, would have occured in a single timestep.

Given that $A$ is positive definite and symmetric, the conjugate gradiant method can be used to compute the solution.
As an added property, $A$ also happens to be diagonally dominate.
This results in $A$ being a M-matrix.
It also means that it could be solved using Bi-Conjugate Gradient Method.
However the Conjugate Gradient method has a faster computation time then Bi-Conjugate Gradiant method for this problem and is used for this reason (\cite{barret1987templates}).

For solving (\ref{equ:C_fixed_point}), the equation can be rearranged into a quadratic form, substituting in $G(C)$ from (\ref{equ:model_functions})
\begin{equation}
  \left(C^{p+1}\right)^2 + \left( \kappa - C^k + \frac{h}{2} \gamma M^{p+1} + \frac{h}{2} \frac{ \gamma C^k M^k}{\kappa + C^k} \right) C^{p+1} + \left( -\kappa C^k + \frac{h}{2} \frac{\gamma \kappa C^k M^k}{\kappa + C^k} \right) = 0.
\end{equation}

Using the quadratic equation results in, 
  \begin{equation} \label{eq:Cquad}
    C^{p+1} = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}
  \end{equation}  
  for which, 
  \begin{equation} \begin{aligned} \label{para:abc}
    a &= 1\\
    b &= \kappa - C^k + \frac{h}{2} \gamma M^{p+1} + \frac{h}{2} \frac{\gamma C^k M^k}{\kappa + C^k} \\
    c &= -\kappa C^k + \frac{h}{2} \frac{\gamma \kappa C^k M^k}{\kappa + C^k}
  \end{aligned}  \end{equation}
  
  To determine which branch of (\ref{eq:Cquad}) to use, a physical situation is used. 
  Specifically the case where there exist no biomass, $M = 0$. 
  %!% "function of $\tau$.... need to change them all to tau at some point....
  The expected outcome is that no substrate is consumed and thus the substrait concentration will remain constant as a function of $t$. 
  When the equations in (\ref{para:abc}) are evaluated at $M = 0$, the result it,
  \begin{equation}
    a = 1, \quad b = \kappa - C^k, \quad c = -\kappa C^k,
  \end{equation} 
  which can be used to evaluate (\ref{eq:Cquad}) as,
  \begin{equation} \begin{aligned}
    C^{p+1} &= \frac{- (\kappa - C^k) \pm \sqrt{(\kappa - C^k)^2 - 4 (-\kappa C^k)}}{2} \\
      &= \frac{1}{2} \left( C^k - \kappa \pm \sqrt{\kappa^2 + 2 \kappa C^k + \left(C^k \right) ^2}\right) \\
      &= \frac{1}{2} \left( C^k - \kappa \pm (\kappa+C^k) \right). \\
  \end{aligned} \end{equation}
  Now, if the positive branch is used the above equation evaluates to $C^{p+1} = C^k$. 
  This means that between any two distinct times, the substrait concentration will remain constants, which was expected. 
  To further this confirmation, the negative branch results in $C^{p+1} = -\kappa $, a non-postive substrate concentration, which is not physically relavent. 
  \begin{equation}
    C^{p+1} = \frac{-b + \sqrt{b^2 - 4ac}}{2a}
  \end{equation} 
  where $a,b$, and $c$ are defined in (\ref{para:abc}).


Now that computable solutions for $M$ and $C$ at a single time step have been found, an algorithm to solve for the next time step can be esstablished.
Algorithm \ref{alg:iterateCM} shows the organizations of solving (\ref{equ:C_fixed_point} - \ref{equ:M_fixed_point}). 
\begin{algorithm}
  \KwData{$M^{p}$ and $C^p$ are temporary solutions defined such that 
    \\$\quad$ $M^p \to M^{k+1}$ and $C^p \to C^{k+1}$ as $p \to P$.
    \\$\quad$ $\epsilon_{sol}$ is a tolerance set for a desired accuracy.}
  \Begin
  {
    \While{convergence is not acheived } 
    {
        %!% Did I ever define b^p ?? If not I should either rewrite this or introduce b^p....
        Solve $A^{(p)} M^{(p+1)} = b^{(p)}$\;
        %!% old....Solve for $M^{p+1}$ using $C^{p}$ and $M^{k}$\;
        Solve $C^{(p+1)} = \frac{1}{2} \left( 2b \pm \sqrt{b^2 - 4c} \right)$\;
        %!% old....Solve for $C^{p+1}$ using $M^{p+1}$, $C^{k}$, and $M^{k}$\;
        Check convergence\; 
        %!% Here I should have (maybe?) M^{k+1} := M^{(p+1)} etc...
        Let $C^{p} = C_{p+1}$\;
        Let $M^{p} = M_{p+1}$\;
        Let $p = p + 1 $\;
    }
  }
  \caption{Algorithm for the fully-implicit solving of (\ref{equ:model_system}) }
  \label{alg:iterateCM}
\end{algorithm}
Note that Algorithm \ref{alg:iterateCM} actually describes both a fully- and semi- implicite method for solving (\ref{equ:model_system}). 
If $P = 1$ then only a single iteration of the algorithm is applied.
This would result in a change similar to how the Gauss-Seidal method changes the Jacobi method; the values used would no longer be updated in a single timestep when $P = 1$.

To use the algorithm, the matrix system was converted into a 1D array by use of a bijective mapping defined as:
\begin{equation}
\begin{array}{c c c c}
  %!% Make sure this actually maps to {1, \ldots, nm} instead of 1.... (n+1)(m+1)
  \pi :& \{ 0, \ldots, n\} \times \{0, \ldots, m\} & \to & \{1, \ldots, nm \} \\
       & (i,j)                                     & \to & \pi(i,j)
\end{array}
\end{equation}
%!% I have a figure for this?!?!?!?
This mapping allows the system to be easily stored in diagonal format, since (\ref{equ:M_matrix_form}) has five distinct diagonals.


