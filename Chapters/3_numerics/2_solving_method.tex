\section{Solving Technique}

Now there exist equations for which $C$ and $M$ can be solved, (\ref{equ:C_time_discret}) and (\ref{equ:M_space_discret}) respectivly.
Using $C^{k}$ and $M^{k}_{i,j}$ as approximations of the solutions for (\ref{equ:model_system}) will allow the system to be solved by computing $C^{k+1}$ and $M^{k+1}_{i,j}$.
However, there are complications with trying to get an explicit formula for $M^{k+1}_{i,j}$ from (\ref{equ:M_space_discret}) because of the dependency on $M$ in $D(M)$. 
To remedy this, a fixed point iteration is introduced.
In a single time step, the solutions for $M$ and $C$ can be solved using the previous time step solution in the follow manner:
\begin{equation} \label{equ:M_fixed_point}
  \frac{M^{(p+1)}_{i,j} - M^{k}_{i,j}}{\Delta t} = 
    \frac{1}{\Delta x^2} \sum_{(s,r) \in \mathbb{A}}
    \left( \frac{D(M^{(p)}_{i+s,j+r}) + D(M^{(p)}_{i,j})}{2} \cdot
    ( M^{(p+1)}_{i+s, j+r} - M^{(p+1)}_{i,j}) \right) 
    + F(C^{(p)}_{i,j}) M^{(p+1)}_{i,j}
\end{equation}
\begin{equation} \label{equ:C_fixed_point}
  \frac{C^{(p+1)} - C^{k}}{\Delta t} = \frac{-1}{2} ( G(C^{(p+1)}) M^{(p+1)} + G(C^{k}) M^{k} )
\end{equation}
%!% does p need to be (p) when not in an equation?
where $(p) \in (0,1,\ldots,P)$.
Note, that the equation for $M^{(p+1)}_{i,j}$ shown in (\ref{equ:M_fixed_point}) refers to the interior points only.
A similar change is done for the boundary points but is not shown due to its complexity.
It is important to show explicitly that the purpose of the fixed point iteration is to link two distinct times with $P$ solutions in between them, such that:
\begin{equation}
  \begin{aligned}
  M^{(p=0)} &= M^{k}, \quad M^{(p=P)} = M^{k+1}, \\
  C^{(p=0)} &= C^{k}, \quad C^{(p=P)} = C^{k+1}.
  \end{aligned}
\end{equation}

In this fixed point format, given by (\ref{equ:M_fixed_point} - \ref{equ:C_fixed_point}), the equations can be rearrange and solved by conventional methods.

For (\ref{equ:M_fixed_point}), a linear system of equations can be created following \cite{saad2003iterativeMethod}.
For each grid point $(i,j)$ a linear system exists, defined as:

\begin{equation} \label{equ:M_linear_equation}
\begin{aligned}
  \frac{M^{k}_{i,j}}{\Delta t} &= 
  \sum_{(i,j) \in \mathbb{A}} \left( \frac{D(M^{(p+1)}_{i+s,j+r}) + D(M^{(p+1)}_{i,j})} 
    {2\Delta x^2} \cdot M^{(p+1)}_{i+s, j+r} \right) \\
  & +\left( \sum_{(i,j) \in \mathbb{A}} \left( \frac{ D(M^{(p+1)}_{i+s,j+r}) + D(M^{(p+1)}_{i,j})} 
    {2\Delta x^2} \right) - F(C^{(p)}_{i,j}) + \frac{1}{\Delta t} \right) M^{(p+1)}_{i,j}.
\end{aligned}
\end{equation} 

From (\ref{equ:M_linear_equation}), a five-diagonal matrix can be created defined as,
%!% Some time find a better way to represent this matrix. Try copying Saad maybe?
\begin{equation} \label{equ:M_matrix_form}
  A = 
    \left( 
      \begin{array}{c c c c c c c c c c}
        a_{i,j} & a_{i+1,j} &  & a_{i,j+1} &   \\
        a_{i-1,j} & \ddots & \ddots &   &  \ddots &   \\
        & \ddots & \ddots & \ddots & & \ddots & \\
        a_{i,j-1} &  & a_{i-1,j} & a_{i,j} & a_{i+1,j} &   &  a_{i,j+1} &   \\
        & \ddots & & \ddots & \ddots & \ddots & & \ddots\\
        & & a_{i,j-1} &  & a_{i-1,j} & a_{i,j} & a_{i+1,j} &  & a_{i,j+1} \\
        & & & \ddots & & \ddots & \ddots & \ddots & \\
        & & & & \ddots & & \ddots & \ddots & a_{i+1,j} \\
        & & & & & a_{i,j-1} & & a_{i-1,j} & a_{i,j}
      \end{array}
    \right)
\end{equation}
where each $a_{i,j}$ is the coefficient based on (\ref{equ:M_linear_equation}). 

Solving (\ref{equ:M_matrix_form}) can be done by use of the Conjugate Gradient method provided that certain conditions are satisfied.

\begin{prop}
  The matrix $A$, (\ref{equ:M_matrix_form}), is positive definite and symmetric when $\frac{1}{ F(C^{(p)}_{i,j}) } < \Delta t$.
\end{prop}
\begin{proof}
  Matrix $A$ is positive definite if all the eigenvalues are positive. 
  Using the Circle theorem described by \cite{gerschgorin1931uber_die_abgrenzung}, the eigenvalues can be shown to be positive if, independently on all rows, the sum of the off-diagonals values is less then the diagonal value.
  This can be verified. From (\ref{equ:M_linear_equation}) it can be said that,
  \begin{equation}
    \sum_{(i,j) \in \mathbb{A}} \left( \frac{D( M^{(p)}_{i+\frac{s}{2}, j+\frac{r}{2}} )}{\Delta x^2} \right)
     < \left( \sum_{(i,j) \in \mathbb{A}} \left( \frac{D( M^{(p)}_{i+\frac{s}{2}, j+\frac{r}{2}} )}{\Delta x^2} \right) 
    - F(C^{(p)}_{i,j}) + \frac{1}{\Delta t} \right). 
  \end{equation}
  This simplifies to,
  \begin{equation}
    F(C^{(p)}_{i,j}) < \frac{1}{\Delta t}
  \end{equation}

  The symmetry of $A$ can be trivially shown if one considers the formation of the diagonals.
  On a single row, each element corresponds to the adjacent grid points of grid $i,j$.
  As the grid ordering counts along, the elements that are equidistance from the diagonal are actually reference to the same grid point. 
  Therefore we have symmetry. 
\end{proof} 

It is important to remark that even though there is a condition for which matrix $A$ is positive definite and symmetric, it realistically will never occur.
The condition, $\frac{1}{F(C)} < \Delta t$, relates the growth of the biomass to the size of timestep selected.
Specifically, if a large enough time step is choosen, then $A$ is not guarenteed to converge.
When this occurs, it means that a time step, larger then the characteristic growth rate of the biomass, has been incorrectly choosen.
This means that the there would be no relavent results since all the growth, and subsequent reactions, would have occured in a single timestep.

Given that $A$ is positive definite and symmetric, the conjugate gradiant method can be used to compute the solution.
As an added property, $A$ also happens to be diagonally dominate.
This results in $A$ being a M-matrix.
It also means that it could be solved using Bi-Conjugate Gradient Method.
However the Conjugate Gradient method has a faster computation time then Bi-Conjugate Gradiant method for this problem and is used for this reason (\cite{barret1987templates}).

For solving (\ref{equ:C_fixed_point}), the equation can be rearranged into a quadratic form, substituting in $G(C)$ from (\ref{equ:model_functions})
\begin{equation} \label{equ:quadratic_C}
  \left(C^{(p+1)}\right)^2 + \left( \kappa - C^k + \frac{h}{2} \gamma M^{(p+1)} + \frac{h}{2} \frac{ \gamma C^k M^k}{\kappa + C^k} \right) C^{(p+1)} + \left( -\kappa C^k + \frac{h}{2} \frac{\gamma \kappa C^k M^k}{\kappa + C^k} \right) = 0.
\end{equation}

Using the quadratic equation results in, 
  \begin{equation} \label{eq:Cquad}
    C^{(p+1)} = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}
  \end{equation}  
  for which, 
  \begin{equation} \begin{aligned} \label{para:abc}
    a &= 1\\
    b &= \kappa - C^k + \frac{h}{2} \gamma M^{(p+1)} + \frac{h}{2} \frac{\gamma C^k M^k}{\kappa + C^k} \\
    c &= -\kappa C^k + \frac{h}{2} \frac{\gamma \kappa C^k M^k}{\kappa + C^k}
  \end{aligned}  \end{equation}

Unless $b^2 - 4ac = 0$, we have two different solutions to $C^{(p+1)}$. 
The problem with that is that if both solutions are positive we have two valid values to be used. 
If the quantitative behaviour of the solutions change with choices of timestep, $h$, then the validity of the method goes down.
Here, we can show that there will always be only one positive solution, regardless of timestep choice.

\begin{prop}
  The quadratic equation defined as (\ref{equ:quadratic_C}) will always have one positive solution and one negative solution for realistic parameter choices.
\end{prop}

\begin{proof}
  Rearranging (\ref{equ:quadratic_C}) so that all the $h$ terms are on the right-hand-side, we get
  \begin{equation}
    \left( C^{(p+1)} \right)^2 + \left(\kappa - C^{k}\right)C^{(p+1)} - \kappa C^{k} =  \left(\frac{\gamma C^k M^k}{2\left(\kappa + C^k\right)} - \left(\frac{\gamma M^{(p+1)}}{2} - \frac{\gamma C^k M^k}{2 \left(\kappa + C^k\right)} \right)C^{(p+1)} \right) h.
  \end{equation}
  To simplify analysis, we let $\bar{a} = \frac{\gamma M^{(p+1)}}{2} - \frac{\gamma C^k M^k}{2\left(\kappa + C^k\right)}$ and $\bar{b} = \frac{\gamma C^k M^k}{2\left(\kappa + C^k\right)}$.
  
  We analysize both the left-hand-side and right-hand-side independently by letting $f_l = \left( C^{(p+1)} \right)^2 + \left(\kappa - C^{k}\right)C^{(p+1)} - \kappa C^{k}$ and $f_r =  \left(\bar{b} - \bar{a} C^{(p+1)} \right) h$.
  $f_l$ is a quadratic equation with positive concavity everywhere and $C^{(p+1)}$-intercept at $-\kappa C^{k} < 0$.
  $f_r$ is a line with a slope opposite to the sign of $\bar{a}$ and has $C^{(p+1)}$-intercept at $\bar{b}h > 0$.
 %!% FINISH THIS UP. NEED TO PROVE THEY WILL ALWAYS INTERCEPT AT BOTH NEG AND POS PINT, and then need to show that the positive intersection cooresponds to the positive branch of the quad equ. 
  Since both $f_l$ and $f_r$ are defined for all $\mathcal{R}$ they must intersect at some point.
  \begin{figure}
    \centering
    \begin{tikzpicture}
      \draw[<->, thick] (-5, 0) -- (5, 0);
      \draw[<->, thick] (0, -3) -- (0, 4); 
      \node[right] at (5, 0) {$C^{(p+1)}$};
      \node[above] at (0, 4) {};

      \draw[<->, domain=-2:4] plot (\x, {0.5*\x*\x - \x - 1});
      \draw (-0.1, -1) -- (0.1, -1);
      \node[right] at (0, -1) {$-\kappa C^{k}$};

      \draw[<->, dashed, domain=-4:4] plot (\x, {-0.5*\x+1});
      \draw (-0.1, 1) -- (0.1, 1); 
      \node[right] at (0, 1) {$\bar{b} h$};

      \draw (7, 2.5) -- (4.5, 2.5) -- (4.5, 4.5) -- (7, 4.5) -- (7, 2.5);
      \draw[<->, dashed] (5, 4) -- (6, 4);
      \node[right] at (6, 4) {$f_l$};
      \draw[<->] (5, 3) -- (6, 3);
      \node[right] at (6, 3) {$f_r$};
    \end{tikzpicture}
  \label{fig:proof_pos_sol}
  \caption{Graph of $f_l = \left( C^{(p+1)} \right)^2 + \left(\kappa - C^{k}\right)C^{(p+1)} - \kappa C^{k}$ and $f_r =  \left(\bar{b} - \bar{a} C^{(p+1)} \right) h$ for $\bar{a} > 0$ and $\kappa - C^{k} < 0$. 
    Notice that because $-\kappa C^{k} < 0$ and $\bar{b} h > 0$ for all realistic parameter values the two functions will always intersect in the positive $C^{(p+1)}$ region.
    Even if $\bar{a} < 0$ or $\kappa - C^{k} > 0$, the functions are guaranteed to have only one positive intersection since the location of the y-intercepts will not qualitativly change.
  }
  \end{figure}
\end{proof}
  
%  To determine which branch of (\ref{eq:Cquad}) to use, a physical situation is used. 
%  Specifically the case where there exist no biomass, $M = 0$. 
%  %!% "function of $\tau$.... need to change them all to tau at some point....
%  The expected outcome is that no substrate is consumed and thus the substrate concentration will remain constant as a function of $t$. 
%  When the equations in (\ref{para:abc}) are evaluated at $M = 0$, the result it,
%  \begin{equation}
%    a = 1, \quad b = \kappa - C^k, \quad c = -\kappa C^k,
%  \end{equation} 
%  which can be used to evaluate (\ref{eq:Cquad}) as,
%  \begin{equation} \begin{aligned}
%    C^{(p+1)} &= \frac{- (\kappa - C^k) \pm \sqrt{(\kappa - C^k)^2 - 4 (-\kappa C^k)}}{2} \\
%      &= \frac{1}{2} \left( C^k - \kappa \pm \sqrt{\kappa^2 + 2 \kappa C^k + \left(C^k \right) ^2}\right) \\
%      &= \frac{1}{2} \left( C^k - \kappa \pm (\kappa+C^k) \right). \\
%  \end{aligned} \end{equation}
%  Now, if the positive branch is used the above equation evaluates to $C^{(p+1)} = C^k$. 
%  This means that between any two distinct times, the substrate concentration will remain constants, which was expected. 
%  To further this confirmation, the negative branch results in $C^{(p+1)} = -\kappa $, a non-postive substrate concentration, which is not physically relavent. 
%  \begin{equation}
%    C^{(p+1)} = \frac{-b + \sqrt{b^2 - 4ac}}{2a}
%  \end{equation} 
%  where $a,b$, and $c$ are defined in (\ref{para:abc}).


Now that computable solutions for $M$ and $C$ at a single time step have been found, an algorithm to solve for the next time step can be established.
Algorithm \ref{alg:iterateCM} shows the organization of solving (\ref{equ:C_fixed_point} - \ref{equ:M_fixed_point}). 
\begin{algorithm}
  \KwData{$M^{k}$, $C^{k}$ are the values from the previous timestep and $p = 0$. }
  \Begin
  {
    Let $M^{(0)} = M^{k}$ and $C^{(0)} = C^{k}$\;
    \While{Convergence is not acheived } 
    {
        %!% Did I ever define b^p ?? If not I should either rewrite this or introduce b^p....
        Solve $A^{(p)} M^{(p+1)} = b^{(p)}$\;
        %!% old....Solve for $M^{(p+1)}$ using $C^{(p)}$ and $M^{k}$\;
        Solve $C^{(p+1)} = \frac{1}{2} \left( 2b \pm \sqrt{b^2 - 4c} \right)$\;
        %!% old....Solve for $C^{(p+1)}$ using $M^{(p+1)}$, $C^{k}$, and $M^{k}$\;
        Check convergence\; 
        %!% Here I should have (maybe?) M^{k+1} := M^{(p+1)} etc...
        Let $C^{(p)} = C_{(p+1)}$\;
        Let $M^{(p)} = M_{(p+1)}$\;
        Let $p = p + 1 $\;
    }
    Let $M^{(k+1)} = M^{(P)}$ and $C^{(k+1)} = C^{(P)}$\;
  }
  \caption{Algorithm for the fully-implicit solving of (\ref{equ:model_system}) }
  \label{alg:iterateCM}
\end{algorithm}

Note that Algorithm \ref{alg:iterateCM} actually describes both a fully- and semi- implicit method for solving (\ref{equ:model_system}). 
If $P = 1$ then only a single iteration of the algorithm is applied, which correlates to a semi-implicit method would behave.
This would result in a change similar to how the Gauss-Seidal method changes the Jacobi method; the values used would no longer be updated in a single timestep when $P = 1$.

To use the algorithm, the matrix system was converted into a 1D array by use of a bijective mapping defined as:
\begin{equation}
\begin{array}{c c c c}
  %!% Make sure this actually maps to {1, \ldots, nm} instead of 1.... (n+1)(m+1)
  \pi :& \{ 0, \ldots, n\} \times \{0, \ldots, m\} & \to & \{0, \ldots, nm \} \\
       & (i,j)                                     & \to & \pi(i,j)
\end{array}
\end{equation}
%!% I have a figure for this?!?!?!?
This mapping allows the system to be easily stored in diagonal format, since (\ref{equ:M_matrix_form}) has five distinct diagonals.


