\section{Computational Setup}

%!% Make this a script and do it for more cores....
\begin{figure}[!htp]
  \centering
  \includegraphics[scale=0.9]{corespeed.eps}
  \caption{A graph showing the computation speed of simulations run with different number of cores.
    The $y$-axis represents the fraction of time each core has in comparison to the $1$-core result.
    The simulation is the same setup as the travelling wave simulation that is defined in the next section.
    The simulation was stopped at $t = 2$ and was run with a grid of $2049 \times 2049$. }
  \label{fig:corespeed}
\end{figure}

The implementation of Algorithm \ref{alg:iterateCM} was done with Fortran. 
The system matrix $A$ in (\ref{equ:M_matrix_form}) is stored in the sparse data format called Compressed Diagonal Storage (CDS) \citep{barret1987templates}.
For each iteration step the linear system is solved using the Conjugate Gradient method \citep{saad2003iterativeMethod}.

All the computations were run on a custom built workstation with an Intel Xeon CPU E5-2650 (1.2 GHz, 20MB cache size) and 32 GB RAM under Red Hat Enterprise Linux Server release 6.5 (Santiago). 
Running the computations with OpenMP, took advantage of 4 out of the 16 threads of the Intel Xeon CPU, with 2 threads to each core.
The choice of 4 cores is that the computational gain for each additional core becomes less significant after 4, as shown in Figure \ref{fig:corespeed}.
The GNU Fortran compiler, version 4.4.7, was used for all computations; the compiler arguments were
\begin{verbatim} -03 -fdefault-real-8 -fopenmp \end{verbatim}



%!% Add other computers nd compilers to show that it was rigerously tested...
